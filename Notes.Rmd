---
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

# Notes

These are my notes for the "Walking Through Postgres" talk at the Ann Arbor Computer Society Meetup on March 6th.

```{r, include=FALSE}
knitr::opts_chunk$set(
  eval = FALSE
)
```

## Installing Postgres

The very first thing we have to do is create a new database cluster.

I'm going to use AWS RDS Postgres to host my database:

[https://console.aws.amazon.com/rds/](https://console.aws.amazon.com/rds/)

While that is starting up I'll show how to run a Postgres database locally.

### System Specfic Instructions

#### macOS

Install using EnterpriseDB, BigSQL, Postgres.app, Fink, MacPorts, or Homebrew

```{bash}
brew install postgres
brew services start postgres
```
#### Windows

EnterpriseDB, BigSQL

#### Ubuntu

Install using EnterpriseDB or PostgreSQL Apt Repository

```{bash}
sudo apt install postgresql
sudo -u postgres psql
```

#### Docker

```{bash}
docker run \
  --name postgres \
  -e POSTGRES_PASSWORD=postgres \
  -v /tmp/postgresdb:/var/lib/postgresql/data \
  postgres:latest
```

If you use Docker to run your database be sure to persist the `PGDATA` directory _outside_ of the Docker container!

### General

The PostgreSQL database cluster is a collection of databases managed by a single server instance.

`pg_ctl` is a utility for initializing a PostgreSQL database cluster, starting, stopping, or restarting the PostgreSQL database server, or displaying the status of a running server.

We can use `pg_ctl` to initialize a new database:

```{bash}
pg_ctl initdb --pgdata=/Users/${USER}/postgres_demo
```

The `--pgdata` flag lets us specify the data directory for this Postgres database.

After we've created the database we can start it:

```{bash}
pg_ctl -D /Users/${USER}/postgres_demo -l logfile start
```

Once our database has started up we can connect to it using the `psql` (default) or `pgcli` command line utilities.

```{bash}
psql postgres_demo
```

By default the PostgreSQL database cluster has several databases and we can view them by specifying the `-l` flag:

```{bash}
psql -X -l
```

Note that I use the `-X` flag to tell `psql` to ignore my startup file (.psqlrc).

#### Creating a new database

We just created a new database cluster, which contains several databases. However in the future we may want to create additional databases.

Use the `createdb` command to create a new database in the PostgreSQL database cluster.

```{bash}
createdb project_db
psql -X -l
```

#### Enabling external connections

So far we've just been interacting with a database running locally, but what if we wanted to configure a database running on a remote server?

There are two important files that need to be modified to allow external clients to connect to the database.

* `postgresql.conf` - Specifies the configuration for the server

* `pg_hba.conf` - Specifies the configuration file for host-based authentication

Modify `pg_hba.conf`:

```{text}
host    project_db  all 0.0.0.0/0   trust
```

Modify `postgresql.conf`:

```{bash}
listen_addresses = '*'
```

Then restart the database

```{bash}
pg_ctl -D postgres_demo restart
```

Now external clients should be able to connect to the database.

Note that you'd typically configure the server to accept connections only from within your network, VPC, load balancer, etc.

## Connecting to Databases

There are a few different ways to form connections using the CLI client, `psql`.

First, if the database is running locally you can just use the database name:

```{bash}
psql postgres_demo
```

Second, you can specify connection parameters using commandline flags:

```{bash}
psql -U user -h localhost -p 5432 -d postgres_demo
```

Third, you can set and use environment variables:

````{bash}
PGDATABASE=postgres_demo psql
```

Environment variables:

* `PGDATABASE`
* `PGHOST`
* `PGPORT`
* `PGUSER`
* `PGPASSWORD` (probably don't specify this one)

Fourth, you can use a connection string:

```{bash}
psql postgres://user:pwd@host:port/dbname
```

Lastly, you can use any combination of these with a .pgpass file.

Add entries to .pgpass file like this:

```{text}
hostname:port:database:username:password
localhost:5432:*:ellis:hunter1
```

You can use `*` to wildcard all or part of a field, for example to use the same username and password to connect to different databases on the same server.

***

Switching back to AWS RDS instance.

# Example: DNS Queries

## Creating roles

Now that we know how to connect to our RDS instance, we should create new users so we don't have to use the administrator user for every connection.

In Postgres there isn't a distinction between users and groups -- both are called roles. A role is an entity that can own database objects and have database privileges; a role can be considered a "user", a "group", or both depending on how it is used.

Roles can have a range of different permissions and can even "inherit" those permissions from other roles.

| Option | Description |
|--------|-------------|
| `SUPERUSER`/`NOSUPERUSER` | Grants the role super user privileges |
| `CREATEDB`/`NOCREATEDB` | Grants the role the ability to create databases |
| `CREATEROLE`/`NOCREATEROLE` | Grants the role the ability to create other roles |
| `INHERIT`/`NOINHERIT` | Roles with `INHERIT` can automatically use whatever database privileges granted to roles it is directly or indirectly a member of. Roles with `NOINHERIT` must use `SET ROLE` |
| `LOGIN`/`NOLOGIN` | Determines whether the role can log in
| `REPLICATION`/`NOREPLICATION` | Determines whether the role can connect to the server in replication mode |
| `BYPASSRLS`/`NOBYPASSRLS` | Determines whether the role bypasses row-level security (RLS) policies |
| `CONNECTION LIMIT` | Number of concurrent connections the role can make |
| `PASSWORD` | Set the role's password |
| `VALID UNTIL 'timestamp'` | Sets a datetime after which the role's password expires |
| `IN ROLE` | Adds the new role as a member of an existing role |
| `ROLE` | Adds an existing role(s) as members of the new role |
| `ADMIN` | Like the `ROLE` option but the named roles are added with the `WITH ADMIN OPTION` |

Here's a role for myself:

```{sql}
CREATE ROLE ellis WITH LOGIN PASSWORD 'hunter1';
```

I want this user to have access to the public schema so we can grant those privileges now:

```{sql}
GRANT ALL PRIVILEGES ON DATABASE aacsdemo to ellis;
```

There are several different kinds of privilege: `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `RULE`, `REFERENCES`, `TRIGGER`, `CREATE`, `TEMPORARY`, `EXECUTE`, and `USAGE`.

We should also create a role for our application to use. The application will have a separate user with lesser permissions:

```{sql}
CREATE ROLE appuser LOGIN PASSWORD 'hunter1' VALID UNTIL '2019-03-07';
GRANT INSERT ON DATABASE aacsdemo to appuser;
```

## Creating schemas

At this point you might want to create a new schema for your data

```{sql}
CREATE SCHEMA myschema;
```

but we're not going to do that today.

## Creating tables

The next step is to setup tables for our data.

We're going to create the following tables:

* `blocks` - IP address blocks
* `locations` - world cities data
* `domains` - domain names resolved to IP address
* `queries` - DNS queries on the client's network

For each DNS query we'll lookup the resolved IP address and then use the IP address to lookup the location.

The application will populate the `domains` and `queries` tables. We will need to supply the data for `blocks` and `locations`. That data comes from MaxMind's GeoIP2 databases, available under the Creative Commons Attribution-ShareAlike 4.0 International License.

[https://dev.maxmind.com/geoip/geoip2/geolite2/](https://dev.maxmind.com/geoip/geoip2/geolite2/)

We can download that data here:

```{bash}
wget -qO- https://geolite.maxmind.com/download/geoip/database/GeoLite2-City-CSV.zip | tar -xvz
head GeoLite2-City-CSV_20190226/GeoLite2-City-Blocks-IPv4.csv
head GeoLite2-City-CSV_20190226/GeoLite2-City-Locations-en.csv
```

Tables are created using the `CREATE TABLE` command, providing the table name and then the column names and types.

Let's first create a table for the locations:

```{sql}
CREATE TABLE locations (
  geoname_id             INTEGER PRIMARY KEY,
  locale_code            VARCHAR(2),
  continent_code         VARCHAR(2),
  continent_name         VARCHAR(13),
  country_iso_code       VARCHAR(2),
  country_name           VARCHAR(44),
  subdivision_1_iso_code VARCHAR(3),
  subdivision_1_name     VARCHAR(52),
  subdivision_2_iso_code VARCHAR(3),
  subdivision_2_name     VARCHAR(38),
  city_name              VARCHAR(63),
  metro_code             INTEGER,
  time_zone              VARCHAR(30),
  is_in_european_union   BOOLEAN
);
```

Next we'll make a table for the network blocks:

```{sql}
CREATE TABLE blocks (
  network                        CIDR UNIQUE NOT NULL,
  geoname_id                     INTEGER REFERENCES locations (geoname_id),
  registered_country_geoname_id  INTEGER,
  represented_country_geoname_id INTEGER,
  is_anonymous_proxy             BOOLEAN,
  is_satellite_provider          BOOLEAN,
  postal_code                    TEXT,
  latitude                       NUMERIC,
  longitude                      NUMERIC,
  accuracy_radius                INTEGER
);
```

Postgres natively supports Classless Inter-Domain Routing (CIDR) as a type! It is better to use these types instead of plain text types to store network addresses, because these types offer input error checking and specialized operators and functions.

CIDR is for *networks* while INET is for hosts, however this is a minor difference.

We'll use INET in the table for domains:

```{sql}
CREATE TABLE domains (
  domain  TEXT PRIMARY KEY,
  address INET,
  created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

Finally we'll create the table to hold queries:

```{sql}
CREATE TABLE queries (
  id        INTEGER PRIMARY KEY,
  timestamp TIMESTAMP,
  type      INTEGER,
  status    INTEGER,
  domain    TEXT,
  client    INET,
  forward   INET
);
```

Now that we've got our tables it's time to start adding data!

## Loading data

Data can inserted in bulk using the `\copy` command. This is much more efficient than `INSERT` when loading large amounts of data.

In this case the files the data are CSV files in a local directory. We can specifiy the delimiter and that the files have a header.

```{sql}
\copy locations FROM 'GeoLite2-City-CSV_20190226/GeoLite2-City-Locations-en.csv' DELIMITER ',' CSV HEADER;
\copy blocks FROM 'GeoLite2-City-CSV_20190226/GeoLite2-City-Blocks-IPv4.csv' DELIMITER ',' CSV HEADER;
\copy blocks FROM 'GeoLite2-City-CSV_20190226/GeoLite2-City-Blocks-IPv6.csv' DELIMITER ',' CSV HEADER;
```

We *must* import the locations first because the blocks depend on it.

This could take up to 3 minutes...

## Indexing

After we've added our data, we can add indexes to the tables. Indexes allow the database server to find and retrieve specific rows much faster than it could do without an index. However indexes add overhead.

We load the data before adding indexes because indexes slow write operations and we wanted to load the data as quickly as possible. In the future we'll be performing fewer write operations at once.

The most basic way to create an index is as follows:

```{sql}
CREATE INDEX blocks_geoname_idx ON blocks (geoname_id);
```

This creates a simple B-tree index on the `geoname_id` column in the `blocks` table.

Sometimes we vant to use a different index type. Postgres provides several index types: B-tree, Hash, GiST, SP-GiST, GIN and BRIN.

| Type | Use case |
|------|----------|
| B-tree | Default type, comparison operators (<, <=, =, >=, >) |
| Hash | Simple equality comparisons, i.e. =, not recommended |
| GiST | Special types, e.g. two-dimensional geometric data, text data |
| SP-GiST | Similar to GiST, non-balanced disk-based data structures e.g. quadtrees, k-d trees, and radix trees |
| GIN | Inverted indexes, multiple values to one row |
| BRIN | Very large tables in which certain columns have some natural correlation with their physical location within the table |

If you don't know which type to use, start with B-tree and then try others.

Because we are working with network addresses we can use a `GIST` index (i.e. Generalized Search Tree):

```{sql}
CREATE INDEX CONCURRENTLY blocks_network_idx ON blocks USING GIST (network inet_ops);
```

We can make similar indexes on other tables:

```{sql}
CREATE UNIQUE INDEX CONCURRENTLY locations_geoname_idx ON locations (geoname_id);
CREATE INDEX CONCURRENTLY locations_country_name_idx ON locations (country_name);

CREATE UNIQUE INDEX CONCURRENTLY domains_domain_idx ON domains (domain);
CREATE INDEX CONCURRENTLY domains_address_idx ON domains USING GIST (address inet_ops);

CREATE UNIQUE INDEX CONCURRENTLY queries_id_idx ON queries (id);
CREATE INDEX CONCURRENTLY queries_domain_idx ON queries (domain);
CREATE INDEX CONCURRENTLY queries_timestamp_idx ON queries (timestamp);
CREATE INDEX CONCURRENTLY queries_day_timestamp_idx ON queries (DATE_TRUNC('d', timestamp));
```

These indexes can drastically improve query performance.

Notice the last index we made includes an expression, `DATE_TRUC('d', timestamp)`, as the target. This is called an _expression_ index, which lets you to index and search the result of a function on the column.

Postgres has support for _partial_ indexes -- indexes that cover only a subset of the table.

```{sql}
CREATE INDEX queries_domain_partial_indx
ON queries (domain)
WHERE status IN (2, 3);
```

Postgres also supports multi-column indexes.

```{sql}
CREATE INDEX queries_domain_date_idx
ON queries (domain, DATE_TRUNC('d', timestamp));
```

Multi-column indexes will be used implicitly if you specify a `WHERE` clause that only includes the first column of a multi-column index.

We now have all of our tables setup, we're done!

## Querying the data

Queries consist of `SELECT` followed by a list of columns or `*`, meaning every column, that come `FROM` a table (or subquery). It is a good idea to limit the number of items returned.

```{sql}
SELECT * FROM queries LIMIT 10;
```

In Postgres we can efficiently sample the table using `TABLESAMPLE SYSTEM (numeric)`, where the numeric is value between 0 and 100 (inclusive). This is not purely random(!) but it is close enough for what we are doing here. For better randomness see `TABLESAMPLE BERNOULLI`.

```{sql}
SELECT * FROM queries TABLESAMPLE SYSTEM (1) LIMIT 10;
```

We can also filter the data using the `WHERE` clause.

Let's find Ann Arbor:

```{sql}
SELECT * FROM locations WHERE city_name = 'Ann Arbor';
```

It is `geoname_id` 4984247.

### Group By and Aggregates

We can calculate simple aggregates:

```{sql}
SELECT COUNT(*) FROM locations;
```

```{sql}
SELECT COUNT(DISTINCT domain) FROM queries;
```

We can calculate aggregates by groups:

```{sql}
SELECT client, COUNT(*) FROM queries GROUP BY client;
```

We can use a sub-query and a window function to get the most recent domain for each client:

```{sql}
SELECT client, timestamp, domain
FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY client ORDER BY "timestamp" DESC) AS rn
  FROM queries TABLESAMPLE SYSTEM (1)
) AS tbl
WHERE rn = 1;
```

### Datetime Functions

Postgres has great support for date/times.

Let's get the current timestamp:

```{sql}
SELECT current_timestamp;
```

and in our local timezone:

```{sql}
SELECT now() AT TIME ZONE 'America/Detroit';
```

Let's get recent queries:

```{sql}
SELECT *
FROM queries
WHERE timestamp >= (current_timestamp - '10 minutes'::INTERVAL);
```

We can also create roll-ups:

```{sql}
SELECT DATE_TRUNC('m', timestamp) AS minute, COUNT(*)
FROM queries TABLESAMPLE SYSTEM (1)
GROUP BY minute;
```

```{sql}
SELECT DATE_TRUNC('hour', timestamp) AS hour, COUNT(*)
FROM queries TABLESAMPLE SYSTEM (1)
GROUP BY hour;
```

### Joins

| Type | Description |
|------|-------------|
| (INNER) JOIN | For each row R1 of T1, the joined table has a row for each row in T2 that satisfies the join condition with R1. |
| LEFT (OUTER) JOIN | First, an inner join is performed. Then, for each row in T1 that does not satisfy the join condition with any row in T2, a joined row is added with null values in columns of T2. Thus, the joined table always has at least one row for each row in T1. |
| RIGHT (OUTER) JOIN | First, an inner join is performed. Then, for each row in T2 that does not satisfy the join condition with any row in T1, a joined row is added with null values in columns of T1. This is the converse of a left join: the result table will always have a row for each row in T2. |
| FULL (OUTER) JOIN | First, an inner join is performed. Then, for each row in T1 that does not satisfy the join condition with any row in T2, a joined row is added with null values in columns of T2. Also, for each row of T2 that does not satisfy the join condition with any row in T1, a joined row with null values in the columns of T1 is added. |
| CROSS JOIN | Every possible combination of rows from T1 and T2 (i.e., a Cartesian product) |

Here we can join a sample from the `blocks` table to the `locations` table and return a subset of their columns:

```{sql}
SELECT
  blocks.network,
  blocks.latitude,
  blocks.longitude,
  locations.country_name,
  locations.city_name
FROM blocks
TABLESAMPLE SYSTEM (1)
JOIN locations ON (blocks.geoname_id=locations.geoname_id)
LIMIT 25;
```

If you want to join tables by the same column name, you can use the `USING` syntax instead of lengthier `ON` syntax:

```{sql}
SELECT network, latitude, longitude, country_name, city_name
FROM blocks TABLESAMPLE SYSTEM (1)
JOIN locations USING (geoname_id)
LIMIT 25;
```

There is also a `NATURAL` join that forms a `USING` list consisting of all column names that appear in both input tables.

```{sql}
SELECT network, latitude, longitude, country_name, city_name
FROM blocks TABLESAMPLE SYSTEM (1)
NATURAL JOIN locations
LIMIT 25;
```

### More Functions

In addition to basic aggregate functions there are many processing functions.

We can do things like split strings:

```{sql}
SELECT string_to_array(domain, '.') AS parts
FROM domains TABLESAMPLE SYSTEM (1)
LIMIT 50;
```

Let's just find the last part of each domain:

```{sql}
SELECT (parts)[array_length(parts,1)] AS top_level
FROM (
  SELECT string_to_array(domain, '.') AS parts
  FROM domains TABLESAMPLE SYSTEM (1)
  LIMIT 50
) AS tbl;
```

and let's count them:

```{sql}
SELECT top_level, COUNT(*) AS n
FROM (
  SELECT (parts)[array_length(parts,1)] AS top_level
  FROM (
    SELECT string_to_array(domain, '.') AS parts
    FROM domains
  ) AS tbl
) AS tbl
GROUP BY top_level
ORDER BY n DESC;
```

### Common Table Expressions

Postgres supports Common Table Expressions (CTEs), which are similar to sub-queries.

```{sql}
WITH top_domains AS (
  SELECT domain, COUNT(id) AS n
  FROM queries
  GROUP BY domain
)
SELECT *
FROM top_domains
ORDER BY n DESC
LIMIT 5;
```

## Network Operators and Functions

Postgres has built-in support for CIDR/INET types but these wouldn't be much good if there weren't functions to go along with them.

For example is this IP address "les than" another one?

```{sql}
SELECT inet '192.168.1.5' < inet '192.168.1.6';
```

We can also check to see if an IP address for subnet inclusion.

```{sql}
SELECT '192.168.1.123'::INET << '192.168.1/24'::CIDR;
```

What is the smallest network which includes both of the given networks?

```{sql}
SELECT inet_merge('192.168.1.5/24', '192.168.2.5/24');
```

These are obviously very powerful tools. Let's try a more complex use case:

Our client might be interested in the location of DNS queries -- do they expect network traffic to foregin countries?

First let's just use a sample of the queries that were not blocked automatically.

```{sql}
SELECT *
FROM queries TABLESAMPLE SYSTEM (1)
WHERE
  status IN (2, 3)
LIMIT 50;
```

Some domains may be duplicated, so let's just combine those and get the count:

```{sql}
SELECT
  domain,
  COUNT(id) AS n
FROM queries TABLESAMPLE SYSTEM (1)
WHERE
  status IN (2, 3)
GROUP BY domain
ORDER BY n DESC
LIMIT 25
```

We can make this a CTE so it is easier to work with:

```{sql}
WITH domain_counts AS (
  SELECT
    domain,
    COUNT(id) AS n
  FROM queries TABLESAMPLE SYSTEM (1)
  WHERE
    status IN (2, 3)
  GROUP BY domain
  ORDER BY n DESC)
SELECT
  domain,
  n,
  address,
  network,
  (latitude, longitude) AS coordiantes,
  locations.country_iso_code,
  city_name AS city,
  subdivision_1_name AS state,
  country_name AS country,
  continent_name AS continent
FROM domain_counts
JOIN domains USING (domain)
JOIN blocks ON (domains.address << blocks.network)
JOIN locations USING (geoname_id)
ORDER BY n DESC;
```

Let's flip this around and find queries to destiations outside North America.

```{sql}
WITH outside_na AS (
  SELECT network, continent_name, country_name
  FROM blocks
  NATURAL JOIN locations
)
SELECT COUNT(*) FROM outside_na;
```

There are a lot of blocks (1,892,859) outside North America.

```{sql}
WITH outside_na AS (
  SELECT network, continent_name, country_name
  FROM blocks
  NATURAL JOIN locations
),
domain_counts AS (
  SELECT
    domain,
    COUNT(id) AS n
  FROM queries TABLESAMPLE SYSTEM (1)
  WHERE
    status IN (2, 3)
  GROUP BY domain
  ORDER BY n DESC)
SELECT country_name, SUM(n) AS n
FROM domain_counts
NATURAL JOIN domains
JOIN outside_na ON (domains.address << outside_na.network)
WHERE continent_name != 'North America'
GROUP BY country_name
ORDER BY n DESC;
```

### JSON(B)

Postgres has great support for JSON(B).

```{sql}
SELECT jsonb_pretty(jsonb_strip_nulls(to_jsonb(locations)))
FROM locations
WHERE city_name='Ann Arbor';
```

```{sql}
SELECT to_json(queries) FROM queries LIMIT 10;
```

Let's just make a new table with our queries as JSON:

```{sql}
CREATE TABLE queriesjson AS
SELECT to_jsonb(queries) AS document FROM queries;
```

Congratulations we just invented NoSQL!

**JSON and JSONB operators:**

| Operator | Description |
|----------|-------------|
| ->[int] | Get JSON array element (indexed from zero, negative integers count from the end) |
| ->[text] | Get JSON object field by key |
| ->>[int] | Get JSON array element as text |
| ->>[text] | Get JSON object field as text |
| #>[text] | Get JSON object at specified path |
| #>>[text] | Get JSON object at specified path as text |

```{sql}
SELECT document->'domain'
FROM queriesjson
LIMIT 5;
```

Convert back to the relational table form:

```{sql}
SELECT (jsonb_populate_record(null::queries, document)).*
FROM queriesjson TABLESAMPLE SYSTEM (1)
LIMIT 10;
```

## Extensions and Geospatial Data

Postgres supports many extensions.

```{sql}
SELECT * FROM pg_available_extensions
```

Remember that we have geospatial data, the lat/lon in the `blocks` table!

```{sql}
CREATE EXTENSION postgis;
```

```{sql}
SELECT ST_MakePoint(longitude, latitude) AS point
FROM blocks TABLESAMPLE SYSTEM (1)
LIMIT 25;
```

```{sql}
WITH points AS (SELECT ST_MakePoint(longitude, latitude) AS point
FROM blocks TABLESAMPLE SYSTEM (1)
LIMIT 25)
SELECT ST_SetSRID(point, 4326) AS point
FROM points;
```

`ST_Distance` returns the minimum 2D Cartesian distance between two geometries in projected units (spatial ref units).

```{sql}
WITH points AS (
  SELECT city_name, ST_SetSRID(ST_MakePoint(longitude, latitude), 4326) AS point
  FROM blocks TABLESAMPLE BERNOULLI (1)
  NATURAL JOIN locations
  LIMIT 5)
SELECT
  pointA.city_name,
  pointB.city_name,
  ST_Distance(pointA.point, pointB.point) AS dist
FROM points AS pointA
CROSS JOIN points AS pointB;
```

We can also cities according to their coordinates using K-Means:

```{sql}
WITH points AS (
  SELECT city_name, ST_SetSRID(ST_MakePoint(longitude, latitude), 4326) AS point
  FROM blocks TABLESAMPLE BERNOULLI (1)
  NATURAL JOIN locations
  WHERE country_name = 'United States'  -- so we recognize the names
  LIMIT 1000),
  clusters AS (
  SELECT ST_ClusterKMeans(point, 10) OVER() AS clusterid, city_name
  FROM points)
SELECT clusterid, array_agg(DISTINCT city_name) AS cities
FROM clusters
GROUP BY clusterid
ORDER BY clusterid;
```

***

# Maintenance

```{sql}
SELECT pg_size_pretty(pg_database_size('aacsdemo'));
SELECT pg_size_pretty(pg_relation_size('queries'));
SELECT relname, n_dead_tup FROM pg_stat_user_tables;
EXPLAIN
SELECT *
FROM blocks
NATURAL JOIN locations;
```

