---
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

# Notes

These are my notes for the "Walking Through Postgres" talk at the Ann Arbor Computer Society Meetup on March 6th.

```{r, include=FALSE}
knitr::opts_chunk$set(
  eval = FALSE
)
```

## Installing Postgres

The very first thing we have to do is create a new database cluster.

I'm going to use AWS RDS Postgres to host my database:

[https://console.aws.amazon.com/rds/](https://console.aws.amazon.com/rds/)

While that is starting up I'll show how to run a Postgres database locally.

### System Specfic Instructions

#### macOS

Install using EnterpriseDB, BigSQL, Postgres.app, Fink, MacPorts, or Homebrew

```{bash}
brew install postgres
brew services start postgres
```
#### Windows

EnterpriseDB, BigSQL

#### Ubuntu

Install using EnterpriseDB or PostgreSQL Apt Repository

```{bash}
sudo apt install postgresql
sudo -u postgres psql
```

#### Docker

```{bash}
docker run \
  --name postgres \
  -e POSTGRES_PASSWORD=postgres \
  -v /tmp/postgresdb:/var/lib/postgresql/data \
  postgres:latest
```

If you use Docker to run your database be sure to persist the `PGDATA` directory _outside_ of the Docker container!

### General

The PostgreSQL database cluster is a collection of databases managed by a single server instance.

`pg_ctl` is a utility for initializing a PostgreSQL database cluster, starting, stopping, or restarting the PostgreSQL database server, or displaying the status of a running server.

We can use `pg_ctl` to initialize a new database:

```{bash}
pg_ctl initdb --pgdata=/Users/${USER}/postgres_demo
```

The `--pgdata` flag lets us specify the data directory for this Postgres database.

After we've created the database we can start it:

```{bash}
pg_ctl -D /Users/${USER}/postgres_demo -l logfile start
```

Once our database has started up we can connect to it using the `psql` (default) or `pgcli` command line utilities.

```{bash}
psql postgres_demo
```

By default the PostgreSQL database cluster has several databases and we can view them by specifying the `-l` flag:

```{bash}
psql -X -l
```

Note that I use the `-X` flag to tell `psql` to ignore my startup file (.psqlrc).

#### Creating a new database

We just created a new database cluster, which contains several databases. However in the future we may want to create additional databases.

Use the `createdb` command to create a new database in the PostgreSQL database cluster.

```{bash}
createdb project_db
psql -X -l
```

#### Enabling external connections

So far we've just been interacting with a database running locally, but what if we wanted to configure a database running on a remote server?

There are two important files that need to be modified to allow external clients to connect to the database.

* `postgresql.conf` - Specifies the configuration for the server

* `pg_hba.conf` - Specifies the configuration file for host-based authentication

Modify `pg_hba.conf`:

```{text}
host    project_db  all 0.0.0.0/0   trust
```

Modify `postgresql.conf`:

```{bash}
listen_addresses = '*'
```

Then restart the database

```{bash}
pg_ctl -D postgres_demo restart
```

Now external clients should be able to connect to the database.

Note that you'd typically configure the server to accept connections only from within your network, VPC, load balancer, etc.

## Connecting to Databases

There are a few different ways to form connections using the CLI client, `psql`.

First, if the database is running locally you can just use the database name:

```{bash}
psql postgres_demo
```

Second, you can specify connection parameters using commandline flags:

```{bash}
psql -U user -h localhost -p 5432 -d postgres_demo
```

Third, you can set and use environment variables:

````{bash}
PGDATABASE=postgres_demo psql
```

Environment variables:

* `PGDATABASE`
* `PGHOST`
* `PGPORT`
* `PGUSER`
* `PGPASSWORD` (probably don't specify this one)

Fourth, you can use a connection string:

```{bash}
psql postgres://user:pwd@host:port/dbname
```

Lastly, you can use any combination of these with a .pgpass file.

Add entries to .pgpass file like this:

```{text}
hostname:port:database:username:password
localhost:5432:*:ellis:hunter1
```

You can use `*` to wildcard all or part of a field, for example to use the same username and password to connect to different databases on the same server.

***

Switching back to AWS RDS instance.

# Example: DNS Queries

## Creating roles

Now that we know how to connect to our RDS instance, we should create new users so we don't have to use the administrator user for every connection.

In Postgres there isn't a distinction between users and groups -- both are called roles. A role is an entity that can own database objects and have database privileges; a role can be considered a "user", a "group", or both depending on how it is used.

Roles can have a range of different permissions and can even "inherit" those permissions from other roles.

| Option | Description |
|--------|-------------|
| `SUPERUSER`/`NOSUPERUSER` | Grants the role super user privileges |
| `CREATEDB`/`NOCREATEDB` | Grants the role the ability to create databases |
| `CREATEROLE`/`NOCREATEROLE` | Grants the role the ability to create other roles |
| `INHERIT`/`NOINHERIT` | Roles with `INHERIT` can automatically use whatever database privileges granted to roles it is directly or indirectly a member of. Roles with `NOINHERIT` must use `SET ROLE` |
| `LOGIN`/`NOLOGIN` | Determines whether the role can log in
| `REPLICATION`/`NOREPLICATION` | Determines whether the role can connect to the server in replication mode |
| `BYPASSRLS`/`NOBYPASSRLS` | Determines whether the role bypasses row-level security (RLS) policies |
| `CONNECTION LIMIT` | Number of concurrent connections the role can make |
| `PASSWORD` | Set the role's password |
| `VALID UNTIL 'timestamp'` | Sets a datetime after which the role's password expires |
| `IN ROLE` | Adds the new role as a member of an existing role |
| `ROLE` | Adds an existing role(s) as members of the new role |
| `ADMIN` | Like the `ROLE` option but the named roles are added with the `WITH ADMIN OPTION` |

Here's a role for myself:

```{sql}
CREATE ROLE ellis WITH LOGIN PASSWORD 'hunter1';
```

I want this user to have access to the public schema so we can grant those privileges now:

```{sql}
GRANT ALL PRIVILEGES ON DATABASE public to ellis;
```

There are several different kinds of privilege: `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `RULE`, `REFERENCES`, `TRIGGER`, `CREATE`, `TEMPORARY`, `EXECUTE`, and `USAGE`.

We should also create a role for our application to use. The application will have a separate user with lesser permissions:

```{sql}
CREATE ROLE appuser LOGIN PASSWORD 'hunter1' VALID UNTIL '2019-03-07';
GRANT INSERT ON DATABASE public to appuser;
```

## Creating schemas

At this point you might want to create a new schema for your data

```{sql}
CREATE SCHEMA myschema;
```

but we're not going to do that.

## Creating tables

The next step is to setup tables for our data.

We're going to create the following tables:

* `blocks` - IP address blocks
* `locations` - world cities data
* `domains` - domain names resolved to IP address
* `queries` - DNS queries on the client's network

For each DNS query we'll lookup the resolved IP address and then use the IP address to lookup the location.

The application will populate the `domains` and `queries` tables. We will need to supply the data for `blocks` and `locations`. That data comes from MaxMind's GeoIP2 databases, available under the Creative Commons Attribution-ShareAlike 4.0 International License.

[https://dev.maxmind.com/geoip/geoip2/geolite2/](https://dev.maxmind.com/geoip/geoip2/geolite2/)

We can download that data here:

```{bash}
wget -qO- https://geolite.maxmind.com/download/geoip/database/GeoLite2-City-CSV.zip | tar -xvz
head GeoLite2-City-CSV_20190226/GeoLite2-City-Blocks-IPv4.csv
head GeoLite2-City-CSV_20190226/GeoLite2-City-Locations-en.csv
```

Tables are created using the `CREATE TABLE` command, providing the table name and then the column names and types.

Let's first create a table for the locations:

```{sql}
CREATE TABLE locations (
  geoname_id             INTEGER PRIMARY KEY,
  locale_code            VARCHAR(2),
  continent_code         VARCHAR(2),
  continent_name         VARCHAR(13),
  country_iso_code       VARCHAR(2),
  country_name           VARCHAR(44),
  subdivision_1_iso_code VARCHAR(3),
  subdivision_1_name     VARCHAR(52),
  subdivision_2_iso_code VARCHAR(3),
  subdivision_2_name     VARCHAR(38),
  city_name              VARCHAR(63),
  metro_code             INTEGER,
  time_zone              VARCHAR(30),
  is_in_european_union   BOOLEAN
);
```

Next we'll make a table for the network blocks:

```{sql}
CREATE TABLE blocks (
  network                        CIDR UNIQUE NOT NULL,
  geoname_id                     INTEGER REFERENCES locations (geoname_id),
  registered_country_geoname_id  INTEGER,
  represented_country_geoname_id INTEGER,
  is_anonymous_proxy             BOOLEAN,
  is_satellite_provider          BOOLEAN,
  postal_code                    TEXT,
  latitude                       NUMERIC,
  longitude                      NUMERIC,
  accuracy_radius                INTEGER
);
```

Postgres natively supports Classless Inter-Domain Routing (CIDR) as a type! It is better to use these types instead of plain text types to store network addresses, because these types offer input error checking and specialized operators and functions.

CIDR is for *networks* while INET is for hosts, however this is a minor difference.

We'll use INET in the table for domains:

```{sql}
CREATE TABLE domains (
  domain  TEXT,
  address INET,
  created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

Finally we'll create the table to hold queries:

```{sql}
CREATE TABLE queries (
  id        INTEGER,
  timestamp TIMESTAMP,
  type      INTEGER,
  status    INTEGER,
  domain    TEXT,
  client    INET,
  forward   INET
);
```

Now that we've got our tables it's time to start adding data!

## Loading data

Data can inserted in bulk using the `\copy` command. This is much more efficient than `INSERT` when loading large amounts of data.

In this case the files the data are CSV files in a local directory. We can specifiy the delimiter and that the files have a header.

```{sql}
\copy blocks FROM 'GeoLite2-City-CSV_20190226/GeoLite2-City-Blocks-IPv4.csv' DELIMITER ',' CSV HEADER;
\copy blocks FROM 'GeoLite2-City-CSV_20190226/GeoLite2-City-Blocks-IPv6.csv' DELIMITER ',' CSV HEADER;
\copy locations FROM 'GeoLite2-City-CSV_20190226/GeoLite2-City-Locations-en.csv' DELIMITER ',' CSV HEADER;
```

## Indexing

After we've added our data, we can add indexes to the tables. Indexes allow the database server to find and retrieve specific rows much faster than it could do without an index. However indexes add overhead.

We load the data before adding indexes because indexes slow write operations and we wanted to load the data as quickly as possible. In the future we'll be performing fewer write operations at once.

The most basic way to create an index is as follows:

```{sql}
CREATE INDEX blocks_geoname_idx ON blocks (geoname_id);
```

This creates a simple B-tree index on the `geoname_id` column in the `blocks` table.

Sometimes we vant to use a different index type. Postgres provides several index types: B-tree, Hash, GiST, SP-GiST, GIN and BRIN.

| Type | Use case |
|------|----------|
| B-tree | Default type, comparison operators (<, <=, =, >=, >) |
| Hash | Simple equality comparisons, i.e. =, not recommended |
| GiST | Special types, e.g. two-dimensional geometric data, text data |
| SP-GiST | Similar to GiST, non-balanced disk-based data structures e.g. quadtrees, k-d trees, and radix trees |
| GIN | Inverted indexes, multiple values to one row |
| BRIN | Very large tables in which certain columns have some natural correlation with their physical location within the table |

If you don't know which type to use, start with B-tree and then try others.

Because we are working with network addresses we can use a `GIST` index (i.e. Generalized Search Tree):

```{sql}
CREATE INDEX blocks_network_idx ON blocks USING GIST (network inet_ops);
```

We can make similar indexes on other tables:

```{sql}
CREATE UNIQUE INDEX locations_geoname_idx ON locations (geoname_id);
CREATE INDEX locations_country_name_idx ON locations (country_name);

CREATE UNIQUE INDEX domains_domain_idx ON domains (domain);
CREATE INDEX domains_address_idx ON domains USING GIST (address);

CREATE UNIQUE INDEX queries_id_idx ON queries (id);
CREATE INDEX queries_domain_idx ON queries (domain);
CREATE INDEX queries_timestamp_idx ON queries (timestamp);
CREATE INDEX queries_day_timestamp_idx ON queries (DATE_TRUNC('d', timestamp));
```

These indexes can drastically improve query performance.

Notice the last index we made includes an expression, `DATE_TRUC('d', timestamp)`, as the target. This is called an _expression_ index, which lets you to index and search the result of a function on the column.

Postgres has support for _partial_ indexes -- indexes that cover only a subset of the table.

```{sql}
CREATE INDEX queries_domain_partial_indx
ON queries (domain)
WHERE status IN (2, 3);
```

Postgres also supports multi-column indexes.

```{sql}
CREATE INDEX queries_domain_date_idx
ON queries (domain, DATE_TRUNC('d', timestamp));
```

Multi-column indexes will be used implicitly if you specify a `WHERE` clause that only includes the first column of a multi-column index.

## DNS Queries



## Querying the data

Queries are easy (until they're not).

```{sql}
SELECT * FROM queries LIMIT 10;
```

### Joins

| Type | Description |
|------|-------------|
| JOIN | Join |
| LEFT JOIN | Join |
| RIGHT JOIN | Join |
| FULL JOIN | Join |
| JOIN | Join |

Join tables:

```{sql}
SELECT *
FROM blocks
JOIN locations ON (blocks.geoname_id=locations.geoname_id);
```

If you want to join tables by the same column name, you can use the `USING` syntax instead of lengthier `ON` syntax:

```{sql}
SELECT *
FROM blocks
JOIN locations USING (geoname_id);
```

###

```{sql}
SELECT
  domain,
  n,
  address,
  network,
  (latitude, longitude) AS coordiantes,
  locations.country_iso_code,
  city_name AS city,
  subdivision_1_name AS state,
  country_name AS country,
  continent_name AS continent
FROM (
  SELECT
    domain,
    COUNT(id) AS n
  FROM queries
  WHERE
    status IN (2, 3)
  GROUP BY domain
  ORDER BY n DESC
  LIMIT 500
) domain_counts
JOIN domains USING (domain)
JOIN blocks ON (domains.address << blocks.network)
JOIN locations USING (geoname_id)
ORDER BY n DESC;
```
